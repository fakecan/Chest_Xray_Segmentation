{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc188a6-5ab8-4ba2-b042-774e480b1439",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92781d-d10b-4ecc-af0c-3240fa5cf773",
   "metadata": {},
   "source": [
    "Input: (256, 256, 3) Chest X-ray image 566개\n",
    "<br>\n",
    "Label: 좌우 lung 마스킹되어 있는 흑백 이미지 566개\n",
    "<br><br>\n",
    "이미지 기준 좌측 폐: Right lung\n",
    "이미지 기준 우측 폐: Left lung\n",
    "<br>\n",
    "※ Task<br>\n",
    "&nbsp;&nbsp;&nbsp;    - Left lung, Right lung 분할(2 classes + 1 class(background)) <br>\n",
    "&nbsp;&nbsp;&nbsp;    - 훈련 & 추론 + 후처리(보간법) 노이즈 제거(예측 성능 ↑)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fabd0b-70f9-413f-829b-39cae4f9a749",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351c35e-e4f0-4e42-9a7c-34a62f85e542",
   "metadata": {},
   "source": [
    "data <br>\n",
    "&nbsp;&nbsp;    └ image/ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0001_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0002_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0003_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        ... <br>\n",
    "&nbsp;&nbsp;    └ label/                     -> 사전 작업 후에는 사용하지 않습니다. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0001_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0002_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        resize_CHNCXR_0003_0.png <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        ...    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6454af-0da1-4586-84f9-0d4b3e508e2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 사전 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c670677-bebb-42ee-89fa-cab07039984f",
   "metadata": {},
   "source": [
    "label 폴더의 마스킹 이미지를 각각 좌측 폐('label_rl/l/*'), 우측 폐'label_rl/r/*'로 분할하여 어노테이션 처리하였습니다.<br>\n",
    "추후 작업 시 라벨링을 배경: 0, 좌측 폐: 1, 우측 폐: 2로 하고 다시 one-hot encoding 작업으로 분할하여 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19d2cd-9fd2-4619-a896-01b1829d5652",
   "metadata": {},
   "source": [
    "# 2. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934d9beb-41ae-49ad-93fb-cc77f6c1e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Input, Dense, Conv2D, Flatten, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce51c14-f320-4b10-a1d7-390b84f52667",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cb9992-f114-4f26-8da2-a6033e7910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproductibility\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# hyper parameter\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "N_CLASSES = 2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8401073-2b70-49d4-af14-c384366929bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input, Label making\n",
    "input_path = './data/image/'\n",
    "input_files = os.listdir(input_path)\n",
    "label_path = './data/label/'\n",
    "label_files = os.listdir(label_path)\n",
    "half_width = int(IMG_WIDTH/2)\n",
    "\n",
    "X_all = np.zeros((len(input_files), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "                  dtype=np.uint8) # Input image\n",
    "y_all = np.zeros((len(label_files), IMG_HEIGHT, IMG_WIDTH, N_CLASSES),\n",
    "                  dtype=np.bool)  # Label(Mask)\n",
    "\n",
    "input_count = 0\n",
    "for input_file in input_files:\n",
    "    input_img = cv2.imread(input_path +  input_file, cv2.IMREAD_GRAYSCALE)\n",
    "    input_img = np.expand_dims(input_img, axis=-1)\n",
    "    \n",
    "    X_all[input_count] = input_img\n",
    "    input_count += 1\n",
    "    \n",
    "\n",
    "cnt_max = []\n",
    "label_count = 0    \n",
    "for label_file in label_files:\n",
    "    label_img = cv2.imread(label_path + label_file, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    cnt, mass = cv2.connectedComponents(label_img)\n",
    "    labels = np.zeros_like(mass) # (256, 256)\n",
    "    \n",
    "    for label_cnt, j in enumerate(range(cnt)):\n",
    "        labels[mass==j] = label_cnt + 10\n",
    "    \n",
    "    labels_info = np.unique(labels, return_counts=True)\n",
    "    labels_index = labels_info[0]\n",
    "    labels_value = labels_info[1]           \n",
    "    \n",
    "    labels_left = labels[:, :half_width]\n",
    "    left_info = np.unique(labels_left, return_counts=True)\n",
    "    left_index = left_info[0]\n",
    "    left_value = left_info[1]\n",
    "    left_sort = left_value.argsort()\n",
    "    left_pick = left_index[left_sort[-2]]\n",
    "    \n",
    "    labels_right = labels[:, half_width:]\n",
    "    right_info = np.unique(labels_right, return_counts=True)\n",
    "    right_index = right_info[0]\n",
    "    right_value = right_info[1]\n",
    "    right_sort = right_value.argsort()\n",
    "    right_pick = right_index[right_sort[-2]]\n",
    "    \n",
    "    labels[labels==left_pick] = 1\n",
    "    labels[labels==right_pick] = 2\n",
    "    labels[((labels != 1) & (labels != 2))] = 0\n",
    "    \n",
    "    labels = tf.keras.utils.to_categorical(y=labels, num_classes=3)\n",
    "    \n",
    "    left_ = np.expand_dims(labels[:,:,1], axis=-1)\n",
    "    right_ = np.expand_dims(labels[:,:,2], axis=-1)\n",
    "    \n",
    "    labels_ = np.concatenate((left_, right_), axis=2)\n",
    "    \n",
    "    y_all[label_count] = labels_\n",
    "    label_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1851910-f527-4441-b18e-755f392ff161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape:  (566, 256, 256, 3)\n",
      "y_all shape:  (566, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_all shape: ', X_all.shape)\n",
    "print('y_all shape: ', y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08a026b-0676-49fc-97c2-ca4d797d5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X_all.astype('float32') / 255.\n",
    "y_all = y_all.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0083a3-97f2-44aa-9485-de53a9207859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (361, 256, 256, 3)\n",
      "X_valid shape:  (91, 256, 256, 3)\n",
      "X_test shape:  (114, 256, 256, 3)\n",
      "y_train shape:  (361, 256, 256, 2)\n",
      "y_valid shape:  (91, 256, 256, 2)\n",
      "y_test shape:  (114, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "# data slicing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_valid shape: ', X_valid.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_valid shape: ', y_valid.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c8d9e4-6296-45ab-91bf-e11d2398b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotTrainData(a,b,c):\n",
    "#     for i in range(3):\n",
    "#         ix = np.random.randint(0, len(a))\n",
    "#         plt.subplot(1,2,1)\n",
    "#         plt.title(\"X_\" + c)\n",
    "#         plt.imshow(a[ix])\n",
    "#         plt.axis('off')\n",
    "#         plt.subplot(1,2,2)\n",
    "#         plt.title(\"y_\" + c)\n",
    "#         plt.imshow(np.squeeze(b[ix]))#, 'gray')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "# plotTrainData(X_train,y_train, 'train')\n",
    "# plotTrainData(X_valid,y_valid, 'valid')\n",
    "# plotTrainData(X_test,y_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1aced-0e47-468c-9168-aacf64c9b9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Modeling(U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8797d-0523-4f2e-b9d2-4db243653abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "![대체 텍스트](https://www.renom.jp/notebooks/tutorial/image_processing/u-net/unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ee53d-04ca-4591-a996-e1ac1e4b0193",
   "metadata": {},
   "source": [
    "## 4-1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0eba74-7c1b-42a0-9f17-d3932072d07b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kwbr1FaF9dUL"
   },
   "outputs": [],
   "source": [
    "# U-Net model\n",
    "# 신경망의 끝단을 MLP가 아닌 CNN을 채택함으로서 기존의 MLP에서의 Flatten으로\n",
    "# 인한 이미지 특성의 보존이 약해지는 것을 보완하고자 하였다.\n",
    "# Input: (H, W, C)\n",
    "# Output: FCN을 사용하여,\n",
    "#         합성곱(분류 클래스 개수, kernel_size, activation)\n",
    "# 이미지 해상도를 Maxpooling(<-> Upsampling)하여 줄여나가다(채널은 증가)\n",
    "# Upsampling으로 작아진 해상도를 늘리며 주변 픽셀을 예측해 값을 채운다.\n",
    "# 가장 특징적인 점은 Skip connection 기법으로 은닉층을 거칠수록 피쳐맵이 형이상학적인 모양을 띄어가는데\n",
    "# 그 이전에 초반 레이어 단에서의 비교적 단순한 피쳐맵(수평선, 수직선, 곡선같은 모양)을 Concatenate하여\n",
    "# 후반 레이어 단에 연결시켜 가중치를 더하는 역할을 한다.\n",
    "def unet(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    \n",
    "    up6 = Concatenate()([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4])\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    \n",
    "    up7 = Concatenate()([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3])\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    \n",
    "    up8 = Concatenate()([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    \n",
    "    up9 = Concatenate()([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "\n",
    "    conv10 = Conv2D(N_CLASSES, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2d60a-6976-4d42-970b-8eebaec30802",
   "metadata": {},
   "source": [
    "## 4-2. Compile & Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17f8567-2090-44d7-8978-a639867c36c6",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgjDwo9DxORy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function 계산\n",
    "# dice coefficient영역이 얼마나 겹치는지를(교집합) 판단하여 오차를 계산한다.(=F1 score)\n",
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ee797-c272-4d01-b3ff-81f896123e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 6/12 [==============>...............] - ETA: 1:01 - loss: 0.7646 - dice_coef: 0.2354"
     ]
    }
   ],
   "source": [
    "model = unet()\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=dice_coef_loss,\n",
    "              optimizer=sgd,\n",
    "              # optimizer='Adam',\n",
    "              metrics=[dice_coef])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# FIT THE MODEL - OPTIMIZATION\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 validation_data=(X_valid, y_valid),\n",
    "                 epochs=EPOCHS,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 verbose=1)\n",
    "#                  callbacks=[modelcheckpoint, earlystopping])\n",
    "\n",
    "runtime = str(datetime.timedelta(seconds=time.time()-start)).split(\".\")\n",
    "runtime = runtime[0]\n",
    "print(f\"Fitting Runtime: {runtime}\")\n",
    "\n",
    "model.save(f\"./model/{time.strftime('%Y%m%d-%H%M%S')}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f95409-c006-49a4-ab77-12a4da5e6075",
   "metadata": {},
   "source": [
    "### i) 모델 존재(로드, 컴파일) ii) 모델 X(훈련, 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd1b64-892d-4f7f-8c6b-3dc57cdb3d29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_h5_path = './model/20211129-151140.h5'\n",
    "# print(f\"model_h5_path:  {model_h5_path}\")\n",
    "\n",
    "if not os.path.isdir('./model'):\n",
    "    os.mkdir('./model')\n",
    "\n",
    "if os.path.isfile(model_h5_path):\n",
    "    print(\"Model already saved and loaded..\")\n",
    "    model = load_model(model_h5_path, custom_objects={'dice_coef':dice_coef, 'dice_coef_loss':dice_coef_loss})\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=dice_coef_loss,\n",
    "                  optimizer=sgd,\n",
    "                  metrics=[dice_coef])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "else: # model.fit\n",
    "    print(\"Model initial setting(compile & fit) start..\")\n",
    "    \n",
    "    # earlystopping = EarlyStopping(monitor='val_loss', # 'dice_coef_loss를 custom 적용.. 찾기'\n",
    "    #                               patience=10)\n",
    "    # modelcheckpoint = ModelCheckpoint(f\"./model_ckpt/{time.strftime('%Y%m%d-%H%M%S')}.h5\",\n",
    "    #                                   monitor='val_loss',\n",
    "    #                                   verbose=1,\n",
    "    #                                   save_best_only=True,\n",
    "    #                                   mode='auto')\n",
    "\n",
    "    # build the model\n",
    "    model = unet()\n",
    "    sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=dice_coef_loss,\n",
    "                  optimizer=sgd,\n",
    "                  # optimizer='Adam',\n",
    "                  metrics=[dice_coef])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # FIT THE MODEL - OPTIMIZATION\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_valid, y_valid),\n",
    "                     epochs=EPOCHS,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     verbose=1)\n",
    "    #                  callbacks=[modelcheckpoint, earlystopping])\n",
    "\n",
    "    runtime = str(datetime.timedelta(seconds=time.time()-start)).split(\".\")\n",
    "    runtime = runtime[0]\n",
    "    print(f\"Fitting Runtime: {runtime}\")\n",
    "\n",
    "    model.save(f\"./model/{time.strftime('%Y%m%d-%H%M%S')}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adfa57-cb3f-4e1e-8cf3-05cf89aeed7c",
   "metadata": {},
   "source": [
    "## 4-3. Fit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771cff3a-201f-461c-abf5-1f8e42013f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(model_h5_path):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # 학습과정 살펴보기\n",
    "    fig, loss_ax = plt.subplots()\n",
    "\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "    acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
    "    acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
    "\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    acc_ax.set_ylabel('dice_coef')\n",
    "\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1296297b-60fa-41bc-8040-c60b90d3763a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yaOK2b8wita"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (361, 256, 256, 3)\n",
      "X_valid (91, 256, 256, 3)\n",
      "X_test (114, 256, 256, 3)\n",
      "y_train (361, 256, 256, 3)\n",
      "y_valid (91, 256, 256, 3)\n",
      "y_test (114, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('X_valid', X_valid.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_valid', y_valid.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752081a1-0789-47fe-bc4c-f850238e48d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf240",
   "language": "python",
   "name": "tf240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
